# どのアルゴリズムを選ぶべきか

- 分類（Classification）：正解となる離散的なカテゴリ（クラス）と入力データの組み合わせで学習し、未知のデータからクラスを予測する
- 回帰：正解となる数値と入力データの組み合わせで学習し、未知のデータから連続値を予測する
- クラスタリング：データを何かしらの基準でグルーピングする
- 次元削減：高次元のデータを可視化や計算量削減などのために低次元マッピングする
- その他
    - 推薦：ユーザーが好みそうなアイテムや、閲覧しているアイテムに類似しているアイテムを提示する
    - 異常検知：不審なアクセスなど、普段とは違う挙動を検知する
    - 頻出パターンマイニング：データ中に高頻度に出現するパターンを抽出する
    - 強化学習：囲碁や将棋のような局所的には正解が不明確な環境で、とるべき行動の方針を学習する


http://scikit-learn.org/stable/tutorial/machine_learning_map/

※ その他（推薦、異常検知、頻出パターンマイニング、強化学習）は覗く

## 分類

- 教師あり学習の1つ
- 予測対象はカテゴリなどの離散的な値を予測する
- メールがスパムかどうかや画像が映っているのがどういった物体かなど
- クラスの数が2の場合を二値分類、3以上の場合を多値分類という

**分類については以下などのアルゴリズムが存在する**

- パーセプトロン
- ロジスティック回帰
- SVM (サポートベクターマシン)
- ニューラルネットワーク
- k-NN (k近傍方、k-Nearest Neighbor Method)
- 決定木
- ランダムフォレスト
- GBDT (Gradient Boosted Decision Tree)
- ナイーブベイズ
- HMM (Hidden Markov Model)

### パーセプトロン

- 様々なアルゴリズムに影響を与えた、歴史的に重要なアルゴリズムである
- パーセプトロンを多層に重ねたものがニューラルネットワークになる

**数式**
数式

- 入力ベクトル(x)と学習した重みベクトル(w)を掛け合わせた値を足して、その値が0以上の時はクラス1、0未満のときはクラス2と分類するシンプルなアルゴリズム
- 図では入力が2つになっているがいくつでも構わない

#### パーセプトロンの特徴

- オンライン学習で学習する
- 予測性能はそこそこで、学習は早い
- 過学習しやすい
- 線形分離可能な問題のみ解ける

##### オンライン学習とバッチ学習

オンライン学習はデータを1つずつ入力して最適化し、バッチ学習は全部を入れて最適化する

##### 過学習

学習に使ったデータに対してはきちんと正解できるが、知らないデータに対してはまったく正解でないという状態になっているモデル
※ 伝統的なパーセプトロンに科学集を抑える仕組みは組み込まれていない

##### 線形分離可能

- パーセプトロンは線形分離可能な問題のみ解くことができる
- 線形分離可能とは、データをある直線で切りよく2つに分けられるデータのことを言う

#### パーセプトロンの決定境界

パーセプトロンの決定境界（線形分離可能）
画像

#### パーセプトロンの仕組み

**計算方法**
sum = x1w1 + x2w2 + x3w3

- x：入力ベクトル
- w：重みベクトル
- t：正解ラベル（1か-1）

パーセプトロンの予測コード
```
import numby as np

def predict(w, x):
    sum = np.dot(w, x)

    if sum >= 0:
        return 1
    else:
        return -1
```

SPAMを例にすると、SPAMの場合は1、非SPAMの場合は-1のようになる

##### どうやって適切な重みベクトルを推定するか

「ある教師データを読み込ませたときの出力がどれくらい期待外れだったか」といった真の値と予測値のズレを表す関数を用いる（損失関数）
今学習している予測モデルがどれくらい良いものかを測る

例）損失関数=（真の値-予測値)2

パーセプトロンの損失関数は、max（0, -twx）というヒンジ損失を使う。ヒンジ損失を使うと、0以下の値を取る時（ご分類されたとき）に損失が大きくなり、正解した時の損失は0となる。予測値が大きく間違えば間違うほど、損失も線形に増えていくのが特徴

パーセプトロンのヒンジ損失関数を使い、全データに対して和を取るコードは以下となる
```
import numpy as np

def perceptron_hinge_loss(w, x, t):
    loss = 0
    for (input, label) in zip(x, t):
        v = label * np.dot(w, input)
        loss += max(0, -v)
    return loss
```

パーセプトロンのヒンジ損失
図

**目的関数**
損失関数をもう少し一般化してどれくらいモデルがデータに合っているのかというのを表す関数

目的関数=損失関数の全データでの和

パラメータである重みベクトルwを推定するには、確率的勾配降下法がよく使われる

目的関数が最も小さいところにたどり着けば、そのパラメータが最適な値となる

確率的勾配法
図

**学習率**
どれくらいの幅でパラメータを修正するかを決めるハイパーパラメータで、修正する幅は学習率×山の傾きで決まる

**ステップ関数**
二値分類では、以下のような関数で、入力の値を+1または-1にしてくれる

ステップ関数
図

**活性化関数**
パーセプトロンにおけるステップ関数のような、出力値を非線形変換する関数


### ロジスティック回帰

- 回帰という名前ではあるが分類のためのアルゴリズムである

#### ロジスティック回帰の特徴

- 出力とは別に、その出力のクラスに所属する確率値が出せる
- 学習はオンライン学習でもバッチ学習でも可能
- 予測性能はまずまず、学習速度は速い
- 過学習を防ぐための正則化項が加わっている

### SVM

### ニューラルネットワーク

### k-NN

### 決定木

### ランダムフォレスト

### GBDT

## 回帰

- 教師あり学習の1つ

**回帰の各アルゴリズム**
- 線形回帰、多項式回帰
- Lasso回帰、Ridge回帰、Elastic Net
- 回帰木
- SVR

## クラスタリング

- 教師なし学習の1つ

## 次元削減

- 高次元のデータからできるだけ情報を保存するように低次元のデータに変換すること

## その他

### 推薦

### 異常検知

### 頻出パターンマイニング

### 強化学習

# まとめ


# 参考
機械学習を始めたくなる！機械学習アルゴリズム解説スライドまとめ
https://freelance.levtech.jp/guide/detail/41/

代表的な機械学習手法一覧
https://qiita.com/tomomoto/items/b3fd1ec7f9b68ab6dfe2
